---
title: "Final Project Fitting Models"
authors: "Anna Livingstone Nate Groves"
date: "Fall 2025"
output:
  pdf_document:
    number_sections: no
    toc: false
    toc_depth: 1
  html_document:
    df_print: paged
    number_sections: no
    toc: false
    toc_depth: 1
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(scipen = 999)
library(tidyverse)
library(corrplot)
library(car)
library(boot)
library(leaps)
library(glmnet)
library(tree)
library(pls)
library(gbm)
libraries <- read_csv("../data/libraries.csv")
```

Define functions so we can remove outliers from training data.
```{r}
# Function to remove outliers using the IQR rule
remove_outliers <- function(x) {
  if (is.numeric(x) && length(unique(na.omit(x))) > 2) {  # skip binary
    q1 <- quantile(x, 0.25, na.rm = TRUE)
    q3 <- quantile(x, 0.75, na.rm = TRUE)
    iqr <- q3 - q1
    lower <- q1 - 1.5 * iqr
    upper <- q3 + 1.5 * iqr
    x[x < lower | x > upper] <- NA  # mark outliers
  }
  return(x)
}

# Columns to ignore
ignore_cols <- c("Bookmobiles", "Branch Library", "state")

# Apply only to numeric, non-binary, and not ignored columns
libraries <- libraries %>%
  mutate(across(
    .cols = where(is.numeric) & !all_of(ignore_cols),
    .fns = remove_outliers
  )) %>%
  drop_na()

# Count occurrences per state
state_counts <- table(libraries$state)

# Find states with < 20 occurrences
rare_states <- names(state_counts[state_counts < 20])

# Combine them into "Other" (this is for CV purposes)
libraries$state <- as.character(libraries$state)
libraries$state[libraries$state %in% rare_states] <- "Other"
libraries$state <- factor(libraries$state)  # re-factor to clean up
# Do the same for testing data:
library_testing <- read_csv("../data/libraries_testing.csv")
library_testing$state <- as.character(library_testing$state)
library_testing$state[library_testing$state %in% rare_states] <- "Other"
library_testing$state[!(library_testing$state %in% libraries$state)] <- "Other"
library_testing$state <- factor(library_testing$state)  # re-factor to clean up
libraries <- libraries %>%
  mutate(across(where(is.character), as.factor))
library_testing <- library_testing %>%
  mutate(across(where(is.character), as.factor))

```

Libraries is already the training set.

# Full Linear Model and Stepwise Selected Linear Model

We can start by using a full linear model:
```{r}
full.library.model <- glm(`Total Circulation` ~ ., data = libraries, family = "gaussian")
summary(full.library.model)

full.model.coeff <- data.frame(
  Variable = names(full.library.model$coefficients),
  Coefficient = full.library.model$coefficients)|>
  filter(Coefficient >=75 | Coefficient <=-75)

ggplot(full.model.coeff)+
  geom_bar(mapping=aes(Variable, Coefficient), stat="identity")+
  theme(axis.text.x = element_text(angle = 90, vjust = 1, hjust=1))+
  ggtitle("Estimated Coefficients: Full Linear Model (Training)")
```

Reference Levels: state- Alaska, Locale- City

The full library model has lots of predictors from the dummy variables of state, 
but we can probably remove some of the predictors. 
```{r}
step(full.library.model, direction="both")
```
The best model uses less predictors, and we can save it: 

```{r}
best.stepwise.model <- glm(formula = `Total Circulation` ~ state + Locale + `Service Area Population` +
    `Percentage of Children's Material Circulation` + `Branch Library` + 
    `Internet Computers` + `Wireless Sessions` + `Children's Program Attendance` + 
    `Local Revenue ($)` + `State Revenue ($)` + `Hours/Year` + 
    `Physical Visits` + `Registered Users` + `Inter-library Loans from Other Library`, 
    data = libraries, family = "gaussian")
summary(best.stepwise.model)
```


```{r}
step.model.coeff <- data.frame(
  Variable = names(best.stepwise.model$coefficients),
  Coefficient = best.stepwise.model$coefficients)|>
  filter(Coefficient >=75 | Coefficient <=-75)

ggplot(step.model.coeff)+
  geom_bar(mapping=aes(Variable, Coefficient), stat="identity")+
  theme(axis.text.x = element_text(angle = 90, vjust = 1, hjust=1))+
  ggtitle("Estimated Coefficients: Stepwise Model (Training)")
```


We can compare the prediction errors for both models using CV:
```{r}
set.seed(13)
cv.glm(libraries, best.stepwise.model, K=100)$delta[2]
set.seed(13)
cv.glm(libraries, full.library.model, K=100)$delta[2]
```
Here, we see that the stepwise model has a lower delta than the full selection model.

# Ridge and LASSO Shrinkage Models

Start by getting the design and response matrices
```{r}
libraries.X <- model.matrix(full.library.model)[,-1]
libraries.Y <- full.library.model$data$`Total Circulation`
```

```{r}
#lets use cross validation and get a ridge and LASSO model
set.seed(4545)
libraries.ridge <- cv.glmnet(x = libraries.X, y=libraries.Y, alpha=0)
plot(libraries.ridge) # 9 recommended for ridge
lambda.ridge <- libraries.ridge$lambda.min
```

LASSO
```{r}

set.seed(4545)
libraries.lasso <- cv.glmnet(x = libraries.X, y=libraries.Y, alpha=1)
plot(libraries.lasso) # 7 recommended for LASSO
lambda.lasso <- libraries.lasso$lambda.min
```

# Tree Methods

```{r}
set.seed(4545)
libraries.tree.data <- libraries

names(libraries.tree.data) <- make.names(names(libraries.tree.data))

tree.libraries <- tree(Total.Circulation ~ . , data = libraries.tree.data)
```

```{r}
set.seed(4545)
cv.tree.libraries <- cv.tree(tree.libraries, K = 10)
which(min(cv.tree.libraries$dev) == cv.tree.libraries$dev)
cv.tree.libraries$size[1] # The cross validation pruning recommends 8 terminal nodes
```

```{r}
par(mfrow = c(1, 2))
plot(cv.tree.libraries$size, cv.tree.libraries$dev, type = "b")
plot(cv.tree.libraries$k, cv.tree.libraries$dev, type = "b")

```
We should probably consider 4 or 8 terminal nodes

```{r}
four.leaf.tree <- prune.tree(tree.libraries, best = 4)
plot(four.leaf.tree)
text(four.leaf.tree, pretty = 0)

```

```{r}
eight.leaf.tree <- prune.tree(tree.libraries, best = 8)
plot(eight.leaf.tree)
text(eight.leaf.tree, pretty = 0, cex=.6)
```

```{r}
set.seed(2025)
boost.libraries <- gbm(`Total.Circulation` ~ ., data = libraries.tree.data,
distribution = "gaussian", n.trees = 500,
interaction.depth = 27, shrinkage = 0.1)
summary(boost.libraries)
```


# PLSR

```{r}
libraries.plsr <- plsr(`Total Circulation` ~ ., data=libraries, scale=T, validation="CV")
sapply(libraries, function(x) length(unique(x)))

summary(libraries.plsr)

validationplot(libraries.plsr)
# 15 comps for minimum adjusted RMSE, but I think that we could less, maybe 11
```



# Model Testing
We can test both linear against the testing data. 
```{r}
full.predictions <- predict(full.library.model, newdata = library_testing, type = "response")
full.residuals <- library_testing$`Total Circulation` - full.predictions
stepwise.predictions <- predict(best.stepwise.model, newdata = library_testing, type = "response")
stepwise.residuals <- library_testing$`Total Circulation` - stepwise.predictions
full.MSE <- mean((full.residuals)^2)
stepwise.MSE <- mean((stepwise.residuals)^2)
print(c(full.MSE, stepwise.MSE))
```
On the testing data, the full model actually performed better than the stepwise
model, going against the CV results.

Let's make a residual plot for the full linear model
```{r}
full.plot.data <- cbind(full.predictions, full.residuals)

ggplot(data = full.plot.data , aes(x = full.predictions, y = full.residuals)) +
      geom_point() +
      geom_hline(yintercept = 0, linetype = "dashed", color = "purple") +
      labs(title = "Full Linear Model Residual Plot",
           x = "Fitted Values",
           y = "Residuals")
```

Now a residual plot for the stepwise model.
```{r}
step.plot.data <- cbind(stepwise.predictions, stepwise.residuals)

ggplot(data = step.plot.data , aes(x = stepwise.predictions, y = stepwise.residuals)) +
      geom_point() +
      geom_hline(yintercept = 0, linetype = "dashed", color = "purple") +
      labs(title = "Stepwise Model Residual Plot",
           x = "Fitted Values",
           y = "Residuals")
```


Now let's test LASSO and ridge
```{r}
X.test <- model.matrix(`Total Circulation` ~ ., data = library_testing)[, -1]
Y.test = library_testing$`Total Circulation`
ridge.predictions = predict(libraries.ridge, newx = X.test, type="response", s = lambda.ridge)
ridge.residuals <- library_testing$`Total Circulation` - ridge.predictions
ridge.MSE <- mean((ridge.residuals)^2)

#organizing data for residual plot
ridge.predictions <- as.data.frame(ridge.predictions)|>
  rename(ridge.predictions=s1)
ridge.residuals <- as.data.frame(ridge.residuals)|>
  rename(residuals=s1)
ridge.plot.data <- cbind(ridge.predictions, ridge.residuals)

ggplot(data = ridge.plot.data , aes(x = ridge.predictions, y = residuals)) +
      geom_point() +
      geom_hline(yintercept = 0, linetype = "dashed", color = "purple") +
      labs(title = "Ridge Regression Residual Plot",
           x = "Fitted Values",
           y = "Residuals")
```

```{r}
lasso.predictions = predict(libraries.lasso, newx = X.test, s = lambda.lasso)
lasso.residuals <- library_testing$`Total Circulation` - lasso.predictions
lasso.MSE <- mean((lasso.residuals)^2)

#organizing data for residual plot
lasso.predictions <- as.data.frame(lasso.predictions)|>
  rename(lasso.predictions=s1)
lasso.residuals <- as.data.frame(lasso.residuals)|>
  rename(residuals=s1)
lasso.plot.data <- cbind(lasso.predictions, lasso.residuals)

ggplot(data = lasso.plot.data , aes(x = lasso.predictions, y = residuals)) +
      geom_point() +
      geom_hline(yintercept = 0, linetype = "dashed", color = "purple") +
      labs(title = "Lasso Regression Residual Plot",
           x = "Fitted Values",
           y = "Residuals")
```


Now test the tree:
```{r}
# fix the testing names so they work for the tree:
libraries.tree.testing.data <- library_testing
names(libraries.tree.testing.data) <- make.names(names(library_testing))
boost.predictions <- predict(boost.libraries, newdata=libraries.tree.testing.data, n.trees = 500)
four.leaf.predictions <- predict(four.leaf.tree, newdata = libraries.tree.testing.data)
four.leaf.residuals <- library_testing$`Total Circulation` - four.leaf.predictions
four.leaf.MSE <- mean((four.leaf.residuals)^2)

tree4.plot.data <- cbind(four.leaf.predictions, four.leaf.residuals)

ggplot(data = tree4.plot.data , aes(x = four.leaf.predictions, y = four.leaf.residuals)) +
      geom_point() +
      geom_hline(yintercept = 0, linetype = "dashed", color = "purple") +
      labs(title = "Tree Model (4 Leaf) Residual Plot",
           x = "Fitted Values",
           y = "Residuals")
```

```{r}
eight.leaf.predictions <- predict(eight.leaf.tree, newdata = libraries.tree.testing.data)
eight.leaf.residuals <- library_testing$`Total Circulation` - eight.leaf.predictions
eight.leaf.MSE <- mean((eight.leaf.residuals)^2)

tree8.plot.data <- cbind(eight.leaf.predictions, eight.leaf.residuals)

ggplot(data = tree8.plot.data , aes(x = eight.leaf.predictions, y = eight.leaf.residuals)) +
      geom_point() +
      geom_hline(yintercept = 0, linetype = "dashed", color = "purple") +
      labs(title = "Tree Model (8 Leaf) Residual Plot",
           x = "Fitted Values",
           y = "Residuals")
```

```{r}
boost.predictions <- predict(boost.libraries, newdata=libraries.tree.testing.data, n.trees = 500)
boost.residuals <- library_testing$`Total Circulation` - boost.predictions
boost.MSE <- mean((boost.residuals)^2)

boost.plot.data <- cbind(boost.predictions, boost.residuals)

ggplot(data = tree8.plot.data , aes(x = boost.predictions, y = boost.residuals)) +
      geom_point() +
      geom_hline(yintercept = 0, linetype = "dashed", color = "purple") +
      labs(title = "Boosted Tree Model Residual Plot",
           x = "Fitted Values",
           y = "Residuals")
```


Finally the PLSR:

11 components
```{r}
plsr.predictions.11.comp <- predict(libraries.plsr, newdata = X.test, ncomp = 11)
plsr.11.residuals <- library_testing$`Total Circulation` - plsr.predictions.11.comp
plsr.11.MSE <- mean((plsr.11.residuals)^2)

plsr.11.plot.data <- cbind(plsr.predictions.11.comp, plsr.11.residuals)

ggplot(data = plsr.11.plot.data , aes(x = plsr.predictions.11.comp, y = plsr.11.residuals)) +
      geom_point() +
      geom_hline(yintercept = 0, linetype = "dashed", color = "purple") +
      labs(title = "PLSR Model Residual Plot (11 Components)",
           x = "Fitted Values",
           y = "Residuals")
```

15 components
```{r}
plsr.predictions.15.comp <- predict(libraries.plsr, newdata = X.test, ncomp = 15)
plsr.15.residuals <- library_testing$`Total Circulation` - plsr.predictions.15.comp
plsr.15.MSE <- mean((plsr.15.residuals)^2)

plsr.15.plot.data <- cbind(plsr.predictions.15.comp, plsr.15.residuals)

ggplot(data = plsr.15.plot.data , aes(x = plsr.predictions.15.comp, y = plsr.15.residuals)) +
      geom_point() +
      geom_hline(yintercept = 0, linetype = "dashed", color = "purple") +
      labs(title = "PLSR Model Residual Plot (15 Components)",
           x = "Fitted Values",
           y = "Residuals")
```
Now we can take all of the predictions and compare them to the actual results:
```{r}
# Start with the true values
results <- tibble(
  true = library_testing$`Total Circulation`,

  full_pred       = full.predictions,
  step_pred       = stepwise.predictions,
  ridge_pred      = as.numeric(ridge.predictions),
  lasso_pred      = as.numeric(lasso.predictions),
  four_leaf_pred  = as.numeric(four.leaf.predictions),
  eight_leaf_pred = as.numeric(eight.leaf.predictions),
  plsr_15_pred    = as.numeric(plsr.predictions.15.comp),
  plsr_11_pred    = as.numeric(plsr.predictions.11.comp),
  boost_pred = as.numeric(boost.predictions)
)

# Compute residuals for each model in tidyverse style
results <- results %>%
  mutate(
    full_resid       = true - full_pred,
    step_resid       = true - step_pred,
    ridge_resid      = true - ridge_pred,
    lasso_resid      = true - lasso_pred,
    four_leaf_resid  = true - four_leaf_pred,
    eight_leaf_resid = true - eight_leaf_pred,
    plsr_15_resid    = true - plsr_15_pred,
    plsr_11_resid    = true - plsr_11_pred,
    boost_resid      = true - boost_pred
  )

```

```{r}
plot_data <- results %>%
  select(true, ends_with("_pred"), ends_with("_resid")) %>%
  pivot_longer(
    cols = -true,
    names_to = c("model", ".value"),
    names_pattern = "(.*)_(pred|resid)"
  )

```

straight comparison plot:
```{r}
rmse_data <- plot_data %>%
  group_by(model) %>%
  summarise(MSE = (mean(resid^2)))

ggplot(rmse_data, aes(x = reorder(model, MSE), y = MSE)) +
  geom_col(fill = "steelblue") +
  coord_flip() +
  labs(
    title = "Model Comparison: MSE",
    x = "Model",
    y = "MSE"
  ) +
  theme_bw()

```
Analysis:
The tree methods are confidently the worst, performing well worse than the other
models. The best performing model is ridge regression, which is not too surprising, 
we knew we were dealing with a dataset with lots of colinearity, so seeing the top
three models being ridge, PLSR with 11 components, and LASSO is not too surprising.

```{r}
norm_shape_data <- results %>%
  select(true, ends_with("_pred")) %>%
  arrange(true) %>% 
  mutate(index = row_number()) %>%
  mutate(across(-index, \(x) (x - min(x)) / (max(x) - min(x)))) %>%
  pivot_longer(
    cols = -index,
    names_to = "series",
    values_to = "value"
  )

ggplot(norm_shape_data, aes(x = index, y = value, color = series)) +
  geom_line(alpha = 0.8) +
  labs(
    title = "Normalized Shape Comparison (0–1 scaling)",
    x = "Sorted Observation Index",
    y = "Scaled Value"
  ) +
  theme_bw()

```
I think this is a cool graph showing the shape of the predictions compare to the predicted values

Let's clean it up by excluding the tree methods which we know did not work well:
```{r}
norm_shape_data <- results %>%
  select(true, ends_with("_pred"), -eight_leaf_pred, -four_leaf_pred, - boost_pred) %>%
  arrange(true) %>% 
  mutate(index = row_number()) %>%
  mutate(across(-index, \(x) (x - min(x)) / (max(x) - min(x)))) %>%
  pivot_longer(
    cols = -index,
    names_to = "series",
    values_to = "value"
  )

ggplot(norm_shape_data, aes(x = index, y = value, color = series)) +
  geom_line(alpha = 0.8) +
  labs(
    title = "Normalized Shape Comparison (0–1 scaling)",
    x = "Sorted Observation Index",
    y = "Scaled Value"
  ) +
  theme_bw()

```


To compare only the top three models:


```{r}
norm_shape_data <- results %>%
  select(true, lasso_pred, ridge_pred, plsr_11_pred) %>%
  arrange(true) %>% 
  mutate(index = row_number()) %>%
  mutate(across(-index, \(x) (x - min(x)) / (max(x) - min(x)))) %>%
  pivot_longer(
    cols = -index,
    names_to = "series",
    values_to = "value"
  )

ggplot(norm_shape_data, aes(x = index, y = value, color = series)) +
  geom_line(alpha = 0.8) +
  labs(
    title = "Normalized Shape Comparison (0–1 scaling)",
    x = "Sorted Observation Index",
    y = "Scaled Value"
  ) +
  theme_bw()

```
```{r}
shape_data <- results %>%
  select(true, ends_with("_pred")) %>%
  arrange(true) %>% 
  mutate(index = row_number()) %>%
  pivot_longer(
    cols = -index,
    names_to = "series",
    values_to = "value"
  )
ggplot(shape_data, aes(x = index, y = log1p(value), color = series)) +
  geom_line(alpha = 0.7) +
  labs(
    title = "Shape Comparison of Predictions (log-transformed)",
    x = "Sorted Observation Index",
    y = "log1p(value)"
  ) +
  theme_bw()
```
```{r}

box_data <- results %>%
  select(true, ends_with("_pred")) %>%
  pivot_longer(
    cols = everything(),
    names_to = "series",
    values_to = "value"
  )

ggplot(box_data, aes(x = series, y = value, fill = series)) +
  geom_boxplot(outlier.alpha = 0.2) +
  scale_y_log10() +
  coord_flip() +
  labs(
    title = "Distribution Comparison (log scale)",
    x = "Series",
    y = "log10(value)"
  ) +
  theme_bw()
```

```{r}


# Reshape to long format
violin_data <- results %>%
  select(true, ends_with("_pred")) %>%
  pivot_longer(
    cols = everything(),
    names_to = "series",
    values_to = "value"
  )

# Violin plot
ggplot(violin_data, aes(x = series, y = value, fill = series)) +
  geom_violin(alpha = 0.7, scale = "width", trim = TRUE) +
  geom_boxplot(width = 0.1, fill = "white", outlier.alpha = 0.2) +
  coord_flip() +
  scale_y_log10() +
  labs(
    title = "Violin Plot (Log10): True vs Model Predictions",
    x = "Series",
    y = "log10(Value)"
  ) +
  theme_bw() +
  theme(legend.position = "none")

```

