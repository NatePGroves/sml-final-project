---
title: "Final Project Fitting Models"
authors: "Anna Livingstone Nate Groves"
date: "Fall 2025"
output:
  pdf_document:
    number_sections: no
    toc: false
    toc_depth: 1
  html_document:
    df_print: paged
    number_sections: no
    toc: false
    toc_depth: 1
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(scipen = 999)
library(tidyverse)
library(corrplot)
library(car)
library(boot)
library(leaps)
library(glmnet)
library(tree)
library(pls)
library(gbm)
libraries <- read_csv("../data/libraries.csv")
```

Define functions so we can remove outliers from training data.
```{r}
# Function to remove outliers using the IQR rule
remove_outliers <- function(x) {
  if (is.numeric(x) && length(unique(na.omit(x))) > 2) {  # skip binary
    q1 <- quantile(x, 0.25, na.rm = TRUE)
    q3 <- quantile(x, 0.75, na.rm = TRUE)
    iqr <- q3 - q1
    lower <- q1 - 1.5 * iqr
    upper <- q3 + 1.5 * iqr
    x[x < lower | x > upper] <- NA  # mark outliers
  }
  return(x)
}

# Columns to ignore
ignore_cols <- c("Bookmobiles", "Branch Library", "state")

# Apply only to numeric, non-binary, and not ignored columns
libraries <- libraries %>%
  mutate(across(
    .cols = where(is.numeric) & !all_of(ignore_cols),
    .fns = remove_outliers
  )) %>%
  drop_na()

# Count occurrences per state
state_counts <- table(libraries$state)

# Find states with < 20 occurrences
rare_states <- names(state_counts[state_counts < 20])

# Combine them into "Other" (this is for CV purposes)
libraries$state <- as.character(libraries$state)
libraries$state[libraries$state %in% rare_states] <- "Other"
libraries$state <- factor(libraries$state)  # re-factor to clean up
# Do the same for testing data:
library_testing <- read_csv("../data/libraries_testing.csv")
library_testing$state <- as.character(library_testing$state)
library_testing$state[library_testing$state %in% rare_states] <- "Other"
library_testing$state[!(library_testing$state %in% libraries$state)] <- "Other"
library_testing$state <- factor(library_testing$state)  # re-factor to clean up
libraries <- libraries %>%
  mutate(across(where(is.character), as.factor))
library_testing <- library_testing %>%
  mutate(across(where(is.character), as.factor))

```

Libraries is already the training set.

# Full Linear Model and Stepwise Selected Linear Model

We can start by using a full linear model:
```{r}
full.library.model <- glm(`Total Circulation` ~ ., data = libraries, family = "gaussian")
summary(full.library.model)

full.model.coeff <- data.frame(
  Variable = names(full.library.model$coefficients),
  Coefficient = full.library.model$coefficients)|>
  filter(Coefficient >=75 | Coefficient <=-75)

ggplot(full.model.coeff)+
  geom_bar(mapping=aes(Variable, Coefficient), stat="identity")+
  theme(axis.text.x = element_text(angle = 90, vjust = 1, hjust=1))+
  ggtitle("Estimated Coefficients: Full Linear Model (Training)")
```

Reference Levels: state- Alaska, Locale- City

The full library model has lots of predictors from the dummy variables of state, 
but we can probably remove some of the predictors. 
```{r}
step(full.library.model, direction="both")
```
The best model uses less predictors, and we can save it: 

```{r}
best.stepwise.model <- glm(formula = `Total Circulation` ~ state + Locale + `Service Area Population` +
    `Percentage of Children's Material Circulation` + `Branch Library` + 
    `Internet Computers` + `Wireless Sessions` + `Children's Program Attendance` + 
    `Local Revenue ($)` + `State Revenue ($)` + `Hours/Year` + 
    `Physical Visits` + `Registered Users` + `Inter-library Loans from Other Library`, 
    data = libraries, family = "gaussian")
summary(best.stepwise.model)
```


```{r}
step.model.coeff <- data.frame(
  Variable = names(best.stepwise.model$coefficients),
  Coefficient = best.stepwise.model$coefficients)|>
  filter(Coefficient >=75 | Coefficient <=-75)

ggplot(step.model.coeff)+
  geom_bar(mapping=aes(Variable, Coefficient), stat="identity")+
  theme(axis.text.x = element_text(angle = 90, vjust = 1, hjust=1))+
  ggtitle("Estimated Coefficients: Stepwise Model (Training)")
```


We can compare the prediction errors for both models using CV:
```{r}
set.seed(13)
cv.glm(libraries, best.stepwise.model, K=100)$delta[2]
set.seed(13)
cv.glm(libraries, full.library.model, K=100)$delta[2]
```
Here, we see that the stepwise model has a lower delta than the full selection model.

# Ridge and LASSO Shrinkage Models

Start by getting the design and response matrices
```{r}
libraries.X <- model.matrix(full.library.model)[,-1]
libraries.Y <- full.library.model$data$`Total Circulation`
```

```{r}
#lets use cross validation and get a ridge and LASSO model
set.seed(4545)
libraries.ridge <- cv.glmnet(x = libraries.X, y=libraries.Y, alpha=0)
plot(libraries.ridge) # 9 recommended for ridge
lambda.ridge <- libraries.ridge$lambda.min
```

LASSO
```{r}

set.seed(4545)
libraries.lasso <- cv.glmnet(x = libraries.X, y=libraries.Y, alpha=1)
plot(libraries.lasso) # 7 recommended for LASSO
lambda.lasso <- libraries.lasso$lambda.min
```

# Tree Methods

```{r}
set.seed(4545)
libraries.tree.data <- libraries

names(libraries.tree.data) <- make.names(names(libraries.tree.data))

tree.libraries <- tree(Total.Circulation ~ . , data = libraries.tree.data)
```

```{r}
set.seed(4545)
cv.tree.libraries <- cv.tree(tree.libraries, K = 10)
which(min(cv.tree.libraries$dev) == cv.tree.libraries$dev)
cv.tree.libraries$size[1] # The cross validation pruning recommends 8 terminal nodes
```

```{r}
par(mfrow = c(1, 2))
plot(cv.tree.libraries$size, cv.tree.libraries$dev, type = "b")
plot(cv.tree.libraries$k, cv.tree.libraries$dev, type = "b")

```
We should probably consider 4 or 8 terminal nodes

```{r}
four.leaf.tree <- prune.tree(tree.libraries, best = 4)
plot(four.leaf.tree)
text(four.leaf.tree, pretty = 0)

```

```{r}
eight.leaf.tree <- prune.tree(tree.libraries, best = 8)
plot(eight.leaf.tree)
text(eight.leaf.tree, pretty = 0, cex=.6)
```

```{r}
set.seed(2025)
boost.libraries <- gbm(`Total.Circulation` ~ ., data = libraries.tree.data,
distribution = "gaussian", n.trees = 500,
interaction.depth = 27, shrinkage = 0.1)
summary(boost.libraries)
```


# PLSR

```{r}
libraries.plsr <- plsr(`Total Circulation` ~ ., data=libraries, scale=T, validation="CV")
sapply(libraries, function(x) length(unique(x)))

summary(libraries.plsr)

validationplot(libraries.plsr)
# 15 comps for minimum adjusted RMSE, but I think that we could less, maybe 11
```



# Model Testing
We can test both linear against the testing data. 
```{r}
full.predictions <- predict(full.library.model, newdata = library_testing, type = "response")
full.residuals <- library_testing$`Total Circulation` - full.predictions
stepwise.predictions <- predict(best.stepwise.model, newdata = library_testing, type = "response")
stepwise.residuals <- library_testing$`Total Circulation` - stepwise.predictions
full.MSE <- mean((full.residuals)^2)
stepwise.MSE <- mean((stepwise.residuals)^2)
print(c(full.MSE, stepwise.MSE))
```
On the testing data, the full model actually performed better than the stepwise
model, going against the CV results.

Let's make a residual plot for the full linear model
```{r}
full.plot.data <- cbind(full.predictions, full.residuals)

ggplot(data = full.plot.data , aes(x = full.predictions, y = full.residuals)) +
      geom_point() +
      geom_hline(yintercept = 0, linetype = "dashed", color = "purple") +
      labs(title = "Full Linear Model Residual Plot",
           x = "Fitted Values",
           y = "Residuals")
```

Now a residual plot for the stepwise model.
```{r}
step.plot.data <- cbind(stepwise.predictions, stepwise.residuals)

ggplot(data = step.plot.data , aes(x = stepwise.predictions, y = stepwise.residuals)) +
      geom_point() +
      geom_hline(yintercept = 0, linetype = "dashed", color = "purple") +
      labs(title = "Stepwise Model Residual Plot",
           x = "Fitted Values",
           y = "Residuals")
```


Now let's test LASSO and ridge
```{r}
X.test <- model.matrix(`Total Circulation` ~ ., data = library_testing)[, -1]
Y.test = library_testing$`Total Circulation`
ridge.predictions = predict(libraries.ridge, newx = X.test, type="response", s = lambda.ridge)
ridge.residuals <- library_testing$`Total Circulation` - ridge.predictions
ridge.MSE <- mean((ridge.residuals)^2)

#organizing data for residual plot
ridge.predictions <- as.data.frame(ridge.predictions)|>
  rename(ridge.predictions=s1)
ridge.residuals <- as.data.frame(ridge.residuals)|>
  rename(residuals=s1)
ridge.plot.data <- cbind(ridge.predictions, ridge.residuals)

ggplot(data = ridge.plot.data , aes(x = ridge.predictions, y = residuals)) +
      geom_point() +
      geom_hline(yintercept = 0, linetype = "dashed", color = "purple") +
      labs(title = "Ridge Regression Residual Plot",
           x = "Fitted Values",
           y = "Residuals")
```

```{r}
lasso.predictions = predict(libraries.lasso, newx = X.test, s = lambda.lasso)
lasso.residuals <- library_testing$`Total Circulation` - lasso.predictions
lasso.MSE <- mean((lasso.residuals)^2)

#organizing data for residual plot
lasso.predictions <- as.data.frame(lasso.predictions)|>
  rename(lasso.predictions=s1)
lasso.residuals <- as.data.frame(lasso.residuals)|>
  rename(residuals=s1)
lasso.plot.data <- cbind(lasso.predictions, lasso.residuals)

ggplot(data = lasso.plot.data , aes(x = lasso.predictions, y = residuals)) +
      geom_point() +
      geom_hline(yintercept = 0, linetype = "dashed", color = "purple") +
      labs(title = "Lasso Regression Residual Plot",
           x = "Fitted Values",
           y = "Residuals")
```


Now test the tree:
```{r}
# fix the testing names so they work for the tree:
libraries.tree.testing.data <- library_testing
names(libraries.tree.testing.data) <- make.names(names(library_testing))
four.leaf.predictions <- predict(four.leaf.tree, newdata = libraries.tree.testing.data)
four.leaf.residuals <- library_testing$`Total Circulation` - four.leaf.predictions
four.leaf.MSE <- mean((four.leaf.residuals)^2)

tree4.plot.data <- cbind(four.leaf.predictions, four.leaf.residuals)

ggplot(data = tree4.plot.data , aes(x = four.leaf.predictions, y = four.leaf.residuals)) +
      geom_point() +
      geom_hline(yintercept = 0, linetype = "dashed", color = "purple") +
      labs(title = "Tree Model (4 Leaf) Residual Plot",
           x = "Fitted Values",
           y = "Residuals")
```

```{r}
eight.leaf.predictions <- predict(eight.leaf.tree, newdata = libraries.tree.testing.data)
eight.leaf.residuals <- library_testing$`Total Circulation` - eight.leaf.predictions
eight.leaf.MSE <- mean((eight.leaf.residuals)^2)

tree8.plot.data <- cbind(eight.leaf.predictions, eight.leaf.residuals)

ggplot(data = tree8.plot.data , aes(x = eight.leaf.predictions, y = eight.leaf.residuals)) +
      geom_point() +
      geom_hline(yintercept = 0, linetype = "dashed", color = "purple") +
      labs(title = "Tree Model (8 Leaf) Residual Plot",
           x = "Fitted Values",
           y = "Residuals")
```

```{r}
boost.predictions <- predict(boost.libraries, newdata=libraries.tree.testing.data, n.trees = 500)
boost.residuals <- library_testing$`Total Circulation` - boost.predictions
boost.MSE <- mean((boost.residuals)^2)

boost.plot.data <- cbind(boost.predictions, boost.residuals)

ggplot(data = tree8.plot.data , aes(x = boost.predictions, y = boost.residuals)) +
      geom_point() +
      geom_hline(yintercept = 0, linetype = "dashed", color = "purple") +
      labs(title = "Boosted Tree Model Residual Plot",
           x = "Fitted Values",
           y = "Residuals")
```


Finally the PLSR:

11 components
```{r}
plsr.predictions.11.comp <- predict(libraries.plsr, newdata = X.test, ncomp = 11)
plsr.11.residuals <- library_testing$`Total Circulation` - plsr.predictions.11.comp
plsr.11.MSE <- mean((plsr.11.residuals)^2)

plsr.11.plot.data <- cbind(plsr.predictions.11.comp, plsr.11.residuals)

ggplot(data = plsr.11.plot.data , aes(x = plsr.predictions.11.comp, y = plsr.11.residuals)) +
      geom_point() +
      geom_hline(yintercept = 0, linetype = "dashed", color = "purple") +
      labs(title = "PLSR Model Residual Plot (11 Components)",
           x = "Fitted Values",
           y = "Residuals")
```

15 components
```{r}
plsr.predictions.15.comp <- predict(libraries.plsr, newdata = X.test, ncomp = 15)
plsr.15.residuals <- library_testing$`Total Circulation` - plsr.predictions.15.comp
plsr.15.MSE <- mean((plsr.15.residuals)^2)

plsr.15.plot.data <- cbind(plsr.predictions.15.comp, plsr.15.residuals)

ggplot(data = plsr.15.plot.data , aes(x = plsr.predictions.15.comp, y = plsr.15.residuals)) +
      geom_point() +
      geom_hline(yintercept = 0, linetype = "dashed", color = "purple") +
      labs(title = "PLSR Model Residual Plot (15 Components)",
           x = "Fitted Values",
           y = "Residuals")
```


Let's compare all the models!
```{r}
boxplot.data1 <- cbind(library_testing$`Total Circulation`, full.predictions, stepwise.predictions, ridge.predictions, lasso.predictions, four.leaf.predictions, eight.leaf.predictions, boost.predictions, plsr.predictions.11.comp, plsr.predictions.15.comp)|>
  rename(Testing="library_testing$`Total Circulation`",
         PLSR.11="Total Circulation.11 comps",
         PLSR.15="Total Circulation.15 comps")

boxplot.data <- boxplot.data1|>
  pivot_longer(cols= c(Testing, full.predictions, stepwise.predictions, ridge.predictions, lasso.predictions, four.leaf.predictions, eight.leaf.predictions, boost.predictions, PLSR.11, PLSR.15),
                             names_to = "Model",
                             values_to = "Prediction")

ggplot(boxplot.data, aes(x = Model, y = Prediction, fill = Model)) +
  geom_boxplot() +
  labs(title = "Distribution of Predictions by Model",
       x = "Model",
       y = "Predictions") +
  theme_minimal()

ggplot(boxplot.data, aes(x = Prediction, fill = Model)) +
      geom_histogram( color = "black") +
      facet_wrap(~ Model) + 
      labs(title = "Model Predictions vs Testing Data Histograms", x = "Value", y = "Frequency") +
      theme_minimal()
```


