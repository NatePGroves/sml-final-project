---
title: "Final Project Fitting Models"
authors: "Anna Livingstone Nate Groves"
date: "Fall 2025"
output:
  pdf_document:
    number_sections: no
    toc: false
    toc_depth: 1
  html_document:
    df_print: paged
    number_sections: no
    toc: false
    toc_depth: 1
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(corrplot)
library(car)
library(boot)
library(leaps)
library(glmnet)
library(tree)
library(pls)
libraries <- read_csv("../data/libraries.csv")
```

Define functions so we can remove outliers from training data.
```{r}
# Function to remove outliers using the IQR rule
remove_outliers <- function(x) {
  if (is.numeric(x) && length(unique(na.omit(x))) > 2) {  # skip binary
    q1 <- quantile(x, 0.25, na.rm = TRUE)
    q3 <- quantile(x, 0.75, na.rm = TRUE)
    iqr <- q3 - q1
    lower <- q1 - 1.5 * iqr
    upper <- q3 + 1.5 * iqr
    x[x < lower | x > upper] <- NA  # mark outliers
  }
  return(x)
}

# Columns to ignore
ignore_cols <- c("Bookmobiles", "Branch Library", "state")

# Apply only to numeric, non-binary, and not ignored columns
libraries <- libraries %>%
  mutate(across(
    .cols = where(is.numeric) & !all_of(ignore_cols),
    .fns = remove_outliers
  )) %>%
  drop_na()

# Count occurrences per state
state_counts <- table(libraries$state)

# Find states with < 20 occurrences
rare_states <- names(state_counts[state_counts < 20])

# Combine them into "Other" (this is for CV purposes)
libraries$state <- as.character(libraries$state)
libraries$state[libraries$state %in% rare_states] <- "Other"
libraries$state <- factor(libraries$state)  # re-factor to clean up
# Do the same for testing data:
library_testing <- read_csv("../data/libraries_testing.csv")
library_testing$state <- as.character(library_testing$state)
library_testing$state[library_testing$state %in% rare_states] <- "Other"
library_testing$state[!(library_testing$state %in% libraries$state)] <- "Other"
library_testing$state <- factor(library_testing$state)  # re-factor to clean up
libraries <- libraries %>%
  mutate(across(where(is.character), as.factor))
library_testing <- library_testing %>%
  mutate(across(where(is.character), as.factor))

```

Libraries is already the training set.

# Full Linear Model and Stepwise Selected Linear Model

We can start by using a full linear model:
```{r}
full.library.model <- glm(`Total Circulation` ~ ., data = libraries, family = "gaussian")
summary(full.library.model)
```


The full library model has lots of predictors from the dummy variables of state, 
but we can probably remove some of the predictors. 
```{r}
step(full.library.model, direction="both")
```
The best model uses less predictors, and we can save it: 

```{r}
best.stepwise.model <- glm(formula = `Total Circulation` ~ state + Locale + `Service Area Population` +
    `Percentage of Children's Material Circulation` + `Branch Library` + 
    `Internet Computers` + `Wireless Sessions` + `Children's Program Attendance` + 
    `Local Revenue ($)` + `State Revenue ($)` + `Hours/Year` + 
    `Physical Visits` + `Registered Users` + `Inter-library Loans from Other Library`, 
    data = libraries, family = "gaussian")

```

We can compare the prediction errors for both models using CV:
```{r}
set.seed(13)
cv.glm(libraries, best.stepwise.model, K=100)$delta[2]
set.seed(13)
cv.glm(libraries, full.library.model, K=100)$delta[2]
```
Here, we see that the stepwise model has a lower delta than the full selection model.

# Ridge and LASSO Shrinkage Models

Start by getting the design and response matrices
```{r}
libraries.X <- model.matrix(full.library.model)[,-1]
libraries.Y <- full.library.model$data$`Total Circulation`

#lets use cross validation and get a ridge and LASSO model
set.seed(4545)
libraries.ridge <- cv.glmnet(x = libraries.X, y=libraries.Y, alpha=0)
plot(libraries.ridge) # 9 recommended for ridge
set.seed(4545)
libraries.lasso <- cv.glmnet(x = libraries.X, y=libraries.Y, alpha=1)
plot(libraries.lasso) # 7 recommended for LASSO

```


# Tree Methods

```{r}
set.seed(4545)
libraries.tree.data <- libraries

names(libraries.tree.data) <- make.names(names(libraries.tree.data))

tree.libraries <- tree(Total.Circulation ~ . , data = libraries.tree.data)
```
```{r}
set.seed(4545)
cv.tree.libraries <- cv.tree(tree.libraries, K = 10)
which(min(cv.tree.libraries$dev) == cv.tree.libraries$dev)
cv.tree.libraries$size[1] # The cross validation pruning recommends 8 terminal nodes
```

```{r}
par(mfrow = c(1, 2))
plot(cv.tree.libraries$size, cv.tree.libraries$dev, type = "b")
plot(cv.tree.libraries$k, cv.tree.libraries$dev, type = "b")

```
We should probably consider 4 or 8 terminal nodes

```{r}
four.leaf.tree <- prune.tree(tree.libraries, best = 4)
eight.leaf.tree <- prune.tree(tree.libraries, best = 8)

```



# PLSR

```{r}
libraries.plsr <- plsr(`Total Circulation` ~ ., data=libraries, scale=T, validation="CV")
sapply(libraries, function(x) length(unique(x)))

summary(libraries.plsr)

validationplot(libraries.plsr)
# 15 comps for minimum adjusted RMSE, but I think that we could less, maybe 11
```

# Model Testing
We can test both linear against the testing data. 
```{r}
full.predictions <- predict(full.library.model, newdata = library_testing, type = "response")
full.residuals <- library_testing$`Total Circulation` - full.predictions
stepwise.predictions <- predict(best.stepwise.model, newdata = library_testing, type = "response")
stepwise.residuals <- library_testing$`Total Circulation` - stepwise.predictions
full.MSE <- mean((full.residuals)^2)
stepwise.MSE <- mean((stepwise.residuals)^2)
print(c(full.MSE, stepwise.MSE))
```
On the testing data, the full model actually performed better than the stepwise
model, going against the CV results.


Now let's test LASSO and ridge
```{r}
X.test <- model.matrix(`Total Circulation` ~ ., data = library_testing)[, -1]
Y.test = library_testing$`Total Circulation`
ridge.predictions = predict(libraries.ridge, newx = X.test, s = 9)
lasso.predictions = predict(libraries.lasso, newx = X.test, s = 7)
```
Now test the tree:
```{r}
# fix the testing names so they work for the tree:
libraries.tree.testing.data <- library_testing
names(libraries.tree.testing.data) <- make.names(names(library_testing))
four.leaf.predictions <- predict(four.leaf.tree, newdata = libraries.tree.testing.data)
eight.leaf.predictions <- predict(eight.leaf.tree, newdata = libraries.tree.testing.data)
```

Finally the PLSR:
```{r}
plsr.predictions.15.comp <- predict(libraries.plsr, newdata = X.test, ncomp = 15)
plsr.predictions.11.comp <- predict(libraries.plsr, newdata = X.test, ncomp = 11)
```



