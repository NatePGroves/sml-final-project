geom_boxplot(outlier.alpha = 0.2) +
scale_y_log10() +
coord_flip() +
labs(
title = "Distribution Comparison (log scale)",
x = "Series",
y = "log10(value)"
) +
theme_bw()
violin_data <- results %>%
select(true, ends_with("_pred")) %>%
pivot_longer(
cols = everything(),
names_to = "series",
values_to = "value"
)
# Violin plot
ggplot(violin_data, aes(x = series, y = value, fill = series)) +
geom_violin(alpha = 0.7, scale = "width", trim = TRUE) +
geom_boxplot(width = 0.1, fill = "white", outlier.alpha = 0.2) +
coord_flip() +
scale_y_log10() +
labs(
title = "Violin Plot (Log10): True vs Model Predictions",
x = "Series",
y = "log10(Value)"
) +
theme_bw() +
theme(legend.position = "none")
plot(fitted(full.library.model), residuals(full.library.model),
xlab = "Fitted Values",
ylab = "Residuals",
main = "Residuals vs Fitted Values Plot")
# Add a horizontal line at y=0 for reference
abline(h = 0, col = "red", lty = 2)
plot(fitted(full.library.model), residuals(full.library.model),
xlab = "Fitted Values",
ylab = "Residuals",
main = "Full Linear Model: Residuals vs Fitted Values Plot")
# Add a horizontal line at y=0 for reference
abline(h = 0, col = "red", lty = 2)
plot(fitted(best.stepwise.model), residuals(best.stepwise.model),
xlab = "Fitted Values",
ylab = "Residuals",
main = "Stepwise Model: Residuals vs Fitted Values Plot")
# Add a horizontal line at y=0 for reference
abline(h = 0, col = "red", lty = 2)
knitr::opts_chunk$set(echo = TRUE)
options(scipen = 999)
library(tidyverse)
library(corrplot)
library(car)
library(boot)
library(leaps)
library(glmnet)
library(tree)
library(pls)
libraries <- read_csv("../data/libraries.csv")
set.seed(2025)
boost.libraries <- gbm(`Total.Circulation` ~ ., data = libraries.tree.data,
distribution = "gaussian", n.trees = 500,
interaction.depth = 27, shrinkage = 0.1)
summary(boost.libraries)
summary(boost.libraries)
boost.predictions <- predict(boost.libraries, newdata=libraries.tree.testing.data, n.trees = 500)
boost.predictions <- predict(boost.libraries, newdata=libraries.tree.testing.data, n.trees = 500)
boost.residuals <- library_testing$`Total Circulation` - boost.predictions
boost.MSE <- mean((boost.residuals)^2)
boost.plot.data <- cbind(boost.predictions, boost.residuals)
ggplot(data = tree8.plot.data , aes(x = boost.predictions, y = boost.residuals)) +
geom_point() +
geom_hline(yintercept = 0, linetype = "dashed", color = "purple") +
labs(title = "Boosted Tree Model Residual Plot",
x = "Fitted Values",
y = "Residuals")
plot(boost.libraries)
text(boost.libraries, pretty = 0, cex=.6)
boost.MSE
plot(cv.tree.libraries$k, cv.tree.libraries$dev, type = "b")
cv.tree.libraries
ggplot(rmse_data, aes(x = reorder(model, MSE), y = MSE)) +
geom_col(fill = "steelblue") +
coord_flip() +
labs(
title = "Model Comparison: MSE",
x = "Model",
y = "MSE"
) +
theme_bw()
plsr.predictions.15.comp
plsr.predictions.11.comp <- predict(libraries.plsr, newdata = X.test, ncomp = 11)
plsr.11.residuals <- library_testing$`Total Circulation` - plsr.predictions.11.comp
plsr.11.MSE <- mean((plsr.11.residuals)^2)
plsr.11.plot.data <- cbind(plsr.predictions.11.comp, plsr.11.residuals)
ggplot(data = plsr.11.plot.data , aes(x = plsr.predictions.11.comp, y = plsr.11.residuals)) +
geom_point() +
geom_hline(yintercept = 0, linetype = "dashed", color = "purple") +
labs(title = "PLSR Model Residual Plot (11 Components)",
x = "Fitted Values",
y = "Residuals")
libraries.plsr <- plsr(`Total Circulation` ~ ., data=libraries, scale=T, validation="CV")
sapply(libraries, function(x) length(unique(x)))
summary(libraries.plsr)
validationplot(libraries.plsr)
# 15 comps for minimum adjusted RMSE, but I think that we could less, maybe 11
summary(libraries.plsr)
validationplot(libraries.plsr)
libraries.plsr <- plsr(`Total Circulation` ~ ., data=libraries, scale=T, validation="CV")
validationplot(libraries.plsr)
libraries.plsr
validationplot(libraries.plsr)
libraries.plsr$validation
validationplot(libraries.plsr, estimate = "MSEP")
libraries.plsr <- plsr(`Total Circulation` ~ ., data=libraries, scale=T, validation="CV",
segments = 10)
sapply(libraries, function(x) length(unique(x)))
summary(libraries.plsr)
validationplot(libraries.plsr, estimate = "MSEP")
validationplot(libraries.plsr, estimate = "MSEP")
validationplot(libraries.plsr)
libraries.plsr <- plsr(`Total Circulation` ~ ., data=libraries, scale=T, validation="CV", segments = 10)
libraries.plsr$validation
summary(libraries.plsr)
libraries.plsr <- plsr(`Total Circulation` ~ ., data=libraries, scale=T, validation="CV", segments = 10, ncomp=20)
sapply(libraries, function(x) length(unique(x)))
summary(libraries.plsr)
libraries_clean <- na.omit(libraries)
libraries.plsr <- plsr(
`Total Circulation` ~ .,
data = libraries_clean,
scale = TRUE,
validation = "CV",
segments = 10
)
validationplot(libraries.plsr, estimate = "MSEP")
names(libraries.plsr$validation)
colSums(is.na(libraries))
sapply(libraries, function(x) length(unique(x)))
libraries_fixed <- libraries |>
mutate(
state = factor(state),
Locale = factor(Locale),
`Central Library` = factor(`Central Library`)
)
libraries.plsr <- plsr(
`Total Circulation` ~ .,
data = libraries_fixed,
scale = TRUE,
ncomp = 20,
validation = "CV",
segments = 10
)
validationplot(libraries.plsr, val.type = "MSEP")
sapply(libraries, sd)
plsr.11.MSE
plsr.predictions.15.comp <- predict(libraries.plsr, newdata = X.test, ncomp = 15)
libraries.plsr <- plsr(`Total Circulation` ~ ., data=libraries, scale=T, validation="CV", segments = 10, ncomp=20)
sapply(libraries, function(x) length(unique(x)))
summary(libraries.plsr)
validationplot(libraries.plsr)
# 15 comps for minimum adjusted RMSE, but I think that we could less, maybe 11
plsr.predictions.15.comp <- predict(libraries.plsr, newdata = X.test, ncomp = 15)
libraries.plsr <- plsr(`Total Circulation` ~ ., data=libraries, scale=T, validation="CV")
plsr.predictions.15.comp <- predict(libraries.plsr, newdata = X.test, ncomp = 15)
plsr.predictions.11.comp <- predict(libraries.plsr, newdata = X.test, ncomp = 11)
libraries.plsr <- plsr(`Total Circulation` ~ ., data=libraries, scale=T, validation="CV")
sapply(libraries, function(x) length(unique(x)))
summary(libraries.plsr)
validationplot(libraries.plsr)
# 15 comps for minimum adjusted RMSE, but I think that we could less, maybe 11
plsr.predictions.11.comp <- predict(libraries.plsr, newdata = X.test, ncomp = 11)
plsr.predictions.15.comp <- predict(libraries.plsr, newdata = X.test, ncomp = 15)
plsr.predictions.15.comp <- predict(libraries.plsr, X.test, ncomp = 15)
plsr.predictions.15.comp <- predict(libraries.plsr, newdata = X.test, ncomp = 15)
knitr::opts_chunk$set(echo = TRUE)
options(scipen = 999)
library(tidyverse)
library(corrplot)
library(car)
library(boot)
library(leaps)
library(glmnet)
library(tree)
library(pls)
libraries <- read_csv("../data/libraries.csv")
# Function to remove outliers using the IQR rule
remove_outliers <- function(x) {
if (is.numeric(x) && length(unique(na.omit(x))) > 2) {  # skip binary
q1 <- quantile(x, 0.25, na.rm = TRUE)
q3 <- quantile(x, 0.75, na.rm = TRUE)
iqr <- q3 - q1
lower <- q1 - 1.5 * iqr
upper <- q3 + 1.5 * iqr
x[x < lower | x > upper] <- NA  # mark outliers
}
return(x)
}
# Columns to ignore
ignore_cols <- c("Bookmobiles", "Branch Library", "state")
# Apply only to numeric, non-binary, and not ignored columns
libraries <- libraries %>%
mutate(across(
.cols = where(is.numeric) & !all_of(ignore_cols),
.fns = remove_outliers
)) %>%
drop_na()
# Count occurrences per state
state_counts <- table(libraries$state)
# Find states with < 20 occurrences
rare_states <- names(state_counts[state_counts < 20])
# Combine them into "Other" (this is for CV purposes)
libraries$state <- as.character(libraries$state)
libraries$state[libraries$state %in% rare_states] <- "Other"
libraries$state <- factor(libraries$state)  # re-factor to clean up
# Do the same for testing data:
library_testing <- read_csv("../data/libraries_testing.csv")
library_testing$state <- as.character(library_testing$state)
library_testing$state[library_testing$state %in% rare_states] <- "Other"
library_testing$state[!(library_testing$state %in% libraries$state)] <- "Other"
library_testing$state <- factor(library_testing$state)  # re-factor to clean up
libraries <- libraries %>%
mutate(across(where(is.character), as.factor))
library_testing <- library_testing %>%
mutate(across(where(is.character), as.factor))
full.library.model <- glm(`Total Circulation` ~ ., data = libraries, family = "gaussian")
summary(full.library.model)
full.model.coeff <- data.frame(
Variable = names(full.library.model$coefficients),
Coefficient = full.library.model$coefficients)|>
filter(Coefficient >=75 | Coefficient <=-75)
ggplot(full.model.coeff)+
geom_bar(mapping=aes(Variable, Coefficient), stat="identity")+
theme(axis.text.x = element_text(angle = 90, vjust = 1, hjust=1))+
ggtitle("Estimated Coefficients: Full Linear Model (Training)")
libraries.plsr <- plsr(`Total Circulation` ~ ., data=libraries, scale=T, validation="CV")
sapply(libraries, function(x) length(unique(x)))
summary(libraries.plsr)
validationplot(libraries.plsr)
# 15 comps for minimum adjusted RMSE, but I think that we could less, maybe 11
plsr.predictions.11.comp <- predict(libraries.plsr, newdata = X.test, ncomp = 11)
X.test <- model.matrix(`Total Circulation` ~ ., data = library_testing)[, -1]
Y.test = library_testing$`Total Circulation`
plsr.predictions.11.comp <- predict(libraries.plsr, newdata = X.test, ncomp = 11)
plsr.11.residuals <- library_testing$`Total Circulation` - plsr.predictions.11.comp
plsr.11.MSE <- mean((plsr.11.residuals)^2)
plsr.11.plot.data <- cbind(plsr.predictions.11.comp, plsr.11.residuals)
ggplot(data = plsr.11.plot.data , aes(x = plsr.predictions.11.comp, y = plsr.11.residuals)) +
geom_point() +
geom_hline(yintercept = 0, linetype = "dashed", color = "purple") +
labs(title = "PLSR Model Residual Plot (11 Components)",
x = "Fitted Values",
y = "Residuals")
plsr.predictions.15.comp <- predict(libraries.plsr, newdata = X.test, ncomp = 15)
plsr.15.residuals <- library_testing$`Total Circulation` - plsr.predictions.15.comp
plsr.15.MSE <- mean((plsr.15.residuals)^2)
plsr.15.plot.data <- cbind(plsr.predictions.15.comp, plsr.15.residuals)
ggplot(data = plsr.15.plot.data , aes(x = plsr.predictions.15.comp, y = plsr.15.residuals)) +
geom_point() +
geom_hline(yintercept = 0, linetype = "dashed", color = "purple") +
labs(title = "PLSR Model Residual Plot (15 Components)",
x = "Fitted Values",
y = "Residuals")
plsr.15.MSE
summary(libraries.plsr)
boxplot.data1 <- cbind(library_testing$`Total Circulation`, full.predictions, stepwise.predictions, ridge.predictions, lasso.predictions, four.leaf.predictions, eight.leaf.predictions, plsr.predictions.11.comp, plsr.predictions.15.comp)|>
rename(Testing="library_testing$`Total Circulation`")
best.stepwise.model <- glm(formula = `Total Circulation` ~ state + Locale + `Service Area Population` +
`Percentage of Children's Material Circulation` + `Branch Library` +
`Internet Computers` + `Wireless Sessions` + `Children's Program Attendance` +
`Local Revenue ($)` + `State Revenue ($)` + `Hours/Year` +
`Physical Visits` + `Registered Users` + `Inter-library Loans from Other Library`,
data = libraries, family = "gaussian")
summary(best.stepwise.model)
set.seed(13)
cv.glm(libraries, best.stepwise.model, K=100)$delta[2]
set.seed(13)
cv.glm(libraries, full.library.model, K=100)$delta[2]
libraries.X <- model.matrix(full.library.model)[,-1]
libraries.Y <- full.library.model$data$`Total Circulation`
#lets use cross validation and get a ridge and LASSO model
set.seed(4545)
libraries.ridge <- cv.glmnet(x = libraries.X, y=libraries.Y, alpha=0)
plot(libraries.ridge) # 9 recommended for ridge
lambda.ridge <- libraries.ridge$lambda.min
set.seed(4545)
libraries.lasso <- cv.glmnet(x = libraries.X, y=libraries.Y, alpha=1)
plot(libraries.lasso) # 7 recommended for LASSO
lambda.lasso <- libraries.lasso$lambda.min
set.seed(4545)
libraries.tree.data <- libraries
names(libraries.tree.data) <- make.names(names(libraries.tree.data))
tree.libraries <- tree(Total.Circulation ~ . , data = libraries.tree.data)
set.seed(4545)
cv.tree.libraries <- cv.tree(tree.libraries, K = 10)
which(min(cv.tree.libraries$dev) == cv.tree.libraries$dev)
cv.tree.libraries$size[1] # The cross validation pruning recommends 8 terminal nodes
par(mfrow = c(1, 2))
plot(cv.tree.libraries$size, cv.tree.libraries$dev, type = "b")
plot(cv.tree.libraries$k, cv.tree.libraries$dev, type = "b")
four.leaf.tree <- prune.tree(tree.libraries, best = 4)
eight.leaf.tree <- prune.tree(tree.libraries, best = 8)
set.seed(2025)
boost.libraries <- gbm(`Total.Circulation` ~ ., data = libraries.tree.data,
distribution = "gaussian", n.trees = 500,
interaction.depth = 27, shrinkage = 0.1)
library(gbm)
set.seed(2025)
boost.libraries <- gbm(`Total.Circulation` ~ ., data = libraries.tree.data,
distribution = "gaussian", n.trees = 500,
interaction.depth = 27, shrinkage = 0.1)
summary(boost.libraries)
knitr::opts_chunk$set(echo = TRUE)
options(scipen = 999)
library(tidyverse)
library(corrplot)
library(car)
library(boot)
library(leaps)
library(glmnet)
library(tree)
library(pls)
library(gbm)
libraries <- read_csv("../data/libraries.csv")
full.predictions <- predict(full.library.model, newdata = library_testing, type = "response")
full.residuals <- library_testing$`Total Circulation` - full.predictions
stepwise.predictions <- predict(best.stepwise.model, newdata = library_testing, type = "response")
stepwise.residuals <- library_testing$`Total Circulation` - stepwise.predictions
full.MSE <- mean((full.residuals)^2)
stepwise.MSE <- mean((stepwise.residuals)^2)
print(c(full.MSE, stepwise.MSE))
X.test <- model.matrix(`Total Circulation` ~ ., data = library_testing)[, -1]
Y.test = library_testing$`Total Circulation`
ridge.predictions = predict(libraries.ridge, newx = X.test, type="response", s = lambda.ridge)
ridge.residuals <- library_testing$`Total Circulation` - ridge.predictions
ridge.MSE <- mean((ridge.residuals)^2)
#organizing data for residual plot
ridge.predictions <- as.data.frame(ridge.predictions)|>
rename(ridge.predictions=s1)
ridge.residuals <- as.data.frame(ridge.residuals)|>
rename(residuals=s1)
ridge.plot.data <- cbind(ridge.predictions, ridge.residuals)
ggplot(data = ridge.plot.data , aes(x = ridge.predictions, y = residuals)) +
geom_point() +
geom_hline(yintercept = 0, linetype = "dashed", color = "purple") +
labs(title = "Ridge Regression Residual Plot",
x = "Fitted Values",
y = "Residuals")
lasso.predictions = predict(libraries.lasso, newx = X.test, s = lambda.lasso)
lasso.residuals <- library_testing$`Total Circulation` - lasso.predictions
lasso.MSE <- mean((lasso.residuals)^2)
#organizing data for residual plot
lasso.predictions <- as.data.frame(lasso.predictions)|>
rename(lasso.predictions=s1)
lasso.residuals <- as.data.frame(lasso.residuals)|>
rename(residuals=s1)
lasso.plot.data <- cbind(lasso.predictions, lasso.residuals)
ggplot(data = lasso.plot.data , aes(x = lasso.predictions, y = residuals)) +
geom_point() +
geom_hline(yintercept = 0, linetype = "dashed", color = "purple") +
labs(title = "Lasso Regression Residual Plot",
x = "Fitted Values",
y = "Residuals")
# fix the testing names so they work for the tree:
libraries.tree.testing.data <- library_testing
names(libraries.tree.testing.data) <- make.names(names(library_testing))
four.leaf.predictions <- predict(four.leaf.tree, newdata = libraries.tree.testing.data)
four.leaf.residuals <- library_testing$`Total Circulation` - four.leaf.predictions
four.leaf.MSE <- mean((four.leaf.residuals)^2)
tree4.plot.data <- cbind(four.leaf.predictions, four.leaf.residuals)
ggplot(data = tree4.plot.data , aes(x = four.leaf.predictions, y = four.leaf.residuals)) +
geom_point() +
geom_hline(yintercept = 0, linetype = "dashed", color = "purple") +
labs(title = "Tree Model (4 Leaf) Residual Plot",
x = "Fitted Values",
y = "Residuals")
eight.leaf.predictions <- predict(eight.leaf.tree, newdata = libraries.tree.testing.data)
eight.leaf.residuals <- library_testing$`Total Circulation` - eight.leaf.predictions
eight.leaf.MSE <- mean((eight.leaf.residuals)^2)
tree8.plot.data <- cbind(eight.leaf.predictions, eight.leaf.residuals)
ggplot(data = tree8.plot.data , aes(x = eight.leaf.predictions, y = eight.leaf.residuals)) +
geom_point() +
geom_hline(yintercept = 0, linetype = "dashed", color = "purple") +
labs(title = "Tree Model (8 Leaf) Residual Plot",
x = "Fitted Values",
y = "Residuals")
boost.predictions <- predict(boost.libraries, newdata=libraries.tree.testing.data, n.trees = 500)
boost.residuals <- library_testing$`Total Circulation` - boost.predictions
boost.MSE <- mean((boost.residuals)^2)
boost.plot.data <- cbind(boost.predictions, boost.residuals)
ggplot(data = tree8.plot.data , aes(x = boost.predictions, y = boost.residuals)) +
geom_point() +
geom_hline(yintercept = 0, linetype = "dashed", color = "purple") +
labs(title = "Boosted Tree Model Residual Plot",
x = "Fitted Values",
y = "Residuals")
boxplot.data1 <- cbind(library_testing$`Total Circulation`, full.predictions, stepwise.predictions, ridge.predictions, lasso.predictions, four.leaf.predictions, eight.leaf.predictions, plsr.predictions.11.comp, plsr.predictions.15.comp)|>
rename(Testing="library_testing$`Total Circulation`")
boxplot.data <- boxplot.data1|>
pivot_longer(cols= c(Testing, full.predictions, stepwise.predictions, ridge.predictions, lasso.predictions, four.leaf.predictions, eight.leaf.predictions, plsr.predictions.11.comp, plsr.predictions.15.comp),
names_to = "Model",
values_to = "Prediction")
plsr.predictions.15.comp
boxplot.data1
boxplot.data1 <- cbind(library_testing$`Total Circulation`, full.predictions, stepwise.predictions, ridge.predictions, lasso.predictions, four.leaf.predictions, eight.leaf.predictions, plsr.predictions.11.comp, plsr.predictions.15.comp)|>
rename(Testing="library_testing$`Total Circulation`",
PLSR.11="Total Circulation.11 comps",
PLSR.15="Total Circulation.15 comps")
boxplot.data <- boxplot.data1|>
pivot_longer(cols= c(Testing, full.predictions, stepwise.predictions, ridge.predictions, lasso.predictions, four.leaf.predictions, eight.leaf.predictions, PLSR.11, PLSR.15),
names_to = "Model",
values_to = "Prediction")
ggplot(boxplot.data, aes(x = Model, y = Prediction, fill = Model)) +
geom_boxplot() +
labs(title = "Distribution of Predictions by Model",
x = "Model",
y = "Predictions") +
theme_minimal()
ggplot(boxplot.data, aes(x = Prediction, fill = Model)) +
geom_histogram( color = "black") +
facet_wrap(~ Model) +
labs(title = "Model Predictions vs Testing Data Histograms", x = "Value", y = "Frequency") +
theme_minimal()
boxplot.data1 <- cbind(library_testing$`Total Circulation`, full.predictions, stepwise.predictions, ridge.predictions, lasso.predictions, four.leaf.predictions, eight.leaf.predictions, boost.predictions, plsr.predictions.11.comp, plsr.predictions.15.comp)|>
rename(Testing="library_testing$`Total Circulation`",
PLSR.11="Total Circulation.11 comps",
PLSR.15="Total Circulation.15 comps")
boxplot.data <- boxplot.data1|>
pivot_longer(cols= c(Testing, full.predictions, stepwise.predictions, ridge.predictions, lasso.predictions, four.leaf.predictions, eight.leaf.predictions, boost.predictions, PLSR.11, PLSR.15),
names_to = "Model",
values_to = "Prediction")
ggplot(boxplot.data, aes(x = Model, y = Prediction, fill = Model)) +
geom_boxplot() +
labs(title = "Distribution of Predictions by Model",
x = "Model",
y = "Predictions") +
theme_minimal()
ggplot(boxplot.data, aes(x = Prediction, fill = Model)) +
geom_histogram( color = "black") +
facet_wrap(~ Model) +
labs(title = "Model Predictions vs Testing Data Histograms", x = "Value", y = "Frequency") +
theme_minimal()
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(corrplot)
library(car)
libraries <- read_csv("../data/libraries.csv")
numeric_libaries <- libraries %>%
select(where(is.numeric))
full_correlation_matrix <- cor(numeric_libaries)
y_cor_matrix<- full_correlation_matrix[, "Total Circulation", drop = FALSE]
y_cor_matrix[order(abs(y_cor_matrix[, 1]), decreasing = TRUE), , drop = FALSE]
full_correlation_matrix
model <- lm(`Total Circulation` ~ ., data = numeric_libaries)
vif(model)
for (col in names(numeric_libaries)) {
hist(
numeric_libaries[[col]],
main = paste("Histogram of", col),
xlab = col,
col = "skyblue",
border = "white"
)
}
# Function to remove outliers using the IQR rule
remove_outliers <- function(x) {
if (is.numeric(x) && length(unique(na.omit(x))) > 2) {  # skip binary
q1 <- quantile(x, 0.25, na.rm = TRUE)
q3 <- quantile(x, 0.75, na.rm = TRUE)
iqr <- q3 - q1
lower <- q1 - 1.5 * iqr
upper <- q3 + 1.5 * iqr
x[x < lower | x > upper] <- NA  # mark outliers
}
return(x)
}
# Columns to ignore
ignore_cols <- c("Bookmobiles", "Branch Library")
# Apply only to numeric, non-binary, and not ignored columns
libraries_no_outliers <- libraries %>%
mutate(across(
.cols = where(is.numeric) & !all_of(ignore_cols),
.fns = remove_outliers
)) %>%
drop_na()
libraries_no_outliers_numeric <- libraries_no_outliers %>% select(where(is.numeric))
for (col in names(libraries_no_outliers_numeric)) {
hist(
libraries_no_outliers_numeric[[col]],
main = paste("Histogram of", col),
xlab = col,
col = "skyblue",
border = "white"
)
}
categorical_data <- libraries %>% select(!where(is.numeric))
# Loop through each categorical column and plot
for (col in names(categorical_data)) {
ggplot(categorical_data, aes_string(x = col)) +
geom_bar(fill = "steelblue", color = "white") +
theme_minimal() +
labs(
title = paste("Bar Chart of", col),
x = col,
y = "Count"
) +
theme(
axis.text.x = element_text(angle = 45, hjust = 1)
) -> p
print(p)
}
vif(model)
git branch
